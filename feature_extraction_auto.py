import re
import os
import glob
import cv2
import numpy as np
import pandas as pd
from mediapipe import tasks
from mediapipe.tasks.python import vision
from mediapipe.tasks.python.vision.core import image as mp_image

# ==========================================================
# 0. ê¸°ë³¸ ì„¤ì •
# ==========================================================
VIDEO_DIR = "./ff/"
OUTPUT_DIR = "./features_xlsx_all/"
os.makedirs(OUTPUT_DIR, exist_ok=True)

FRAME_INTERVAL = 1  # ëª¨ë“  í”„ë ˆì„ ë¶„ì„
VISIBILITY_MIN = 0.5  # hip landmark visibility threshold
HIP_RECOGNITION_RATIO_MIN = 0.30  # ê³¨ë°˜ ì¸ì‹ë¥  30% ë¯¸ë§Œì´ë©´ ì œì™¸
SMOOTH_WINDOW = 5  # acc/jerk ê³„ì‚°ìš© ì´ë™í‰ê·  ì°½ (í™€ìˆ˜ ê¶Œì¥)
POSE_MODEL_PATH = "./models/pose_landmarker_full.task"


# ==========================================================
# 1. id, label
# ==========================================================
def extract_id_and_label(video_path):
    fname = os.path.basename(video_path)
    stem = os.path.splitext(fname)[0]

    # Label ì¶”ì¶œ (ì–¸ë”ë°” ì‚¬ì´ì˜ ìˆ«ì)
    m = re.search(r'_(\d)_', stem)
    label = int(m.group(1)) if m else None

    # ID ìƒì„±: ì• ìˆ«ì + "_" + label
    parts = stem.split('_')
    if len(parts) >= 1 and label is not None:
        video_id = f"{parts[0]}_{label}"
    else:
        video_id = stem

    return video_id, label


# ==========================================================
# 2. Missing ì²˜ë¦¬
# ==========================================================
def fill_missing(arr):
    """ë³´ê°„ í›„ forward/backward fillë¡œ ì™„ì „íˆ ì±„ìš°ê¸°"""
    s = pd.Series(arr, dtype=float)
    s = s.interpolate(limit_direction="both").ffill().bfill()
    return s.to_numpy()


def nan_ratio(arr):
    return float(np.mean(np.isnan(np.asarray(arr, dtype=float))))


# ==========================================================
# 3. Kinematics ê³„ì‚° í•¨ìˆ˜
# ==========================================================
def center_point(p1, p2):
    """ë‘ ì ì˜ ì¤‘ì‹¬ì  ê³„ì‚°"""
    return ((p1[0] + p2[0]) / 2.0, (p1[1] + p2[1]) / 2.0)


def velocity_series(pts, dt):
    """ì†ë„ ì‹œê³„ì—´ ê³„ì‚° (ì²« í”„ë ˆì„ ì†ë„ëŠ” 0)"""
    v = [0.0]
    for i in range(1, len(pts)):
        dx = pts[i][0] - pts[i - 1][0]
        dy = pts[i][1] - pts[i - 1][1]
        v.append(np.sqrt(dx ** 2 + dy ** 2) / dt)
    return np.array(v)


def acc_series(v, dt):
    """ê°€ì†ë„ ê³„ì‚°"""
    return np.gradient(v) / dt


def jerk_series(a, dt):
    """ì €í¬ ê³„ì‚°"""
    return np.gradient(a) / dt


def moving_average(arr, window):
    """ê°„ë‹¨ ì´ë™í‰ê·  (ì°½ì´ í¬ë©´ ë” ë¶€ë“œëŸ½ê²Œ)"""
    if window <= 1:
        return np.asarray(arr, dtype=float)
    arr = np.asarray(arr, dtype=float)
    if arr.size < window:
        return arr
    kernel = np.ones(window, dtype=float) / window
    return np.convolve(arr, kernel, mode="same")


def robust_body_size_from_landmarks(lm, visibility_min):
    """Robustí•œ body size ê³„ì‚° - ì–´ê¹¨í­/ê³¨ë°˜í­ë§Œ ì‚¬ìš©, visibility ê¸°ì¤€ ì ìš©"""
    def dist(i, j):
        dx = lm[i].x - lm[j].x
        dy = lm[i].y - lm[j].y
        return np.sqrt(dx * dx + dy * dy)

    pairs = [(11, 12), (23, 24)]
    dists = []

    for i, j in pairs:
        if lm[i].visibility >= visibility_min and lm[j].visibility >= visibility_min:
            d = dist(i, j)
            if np.isfinite(d) and d > 1e-6:
                dists.append(d)

    if len(dists) == 0:
        return np.nan

    bs = float(np.mean(dists))
    return bs


# ==========================================================
# 4. Feature Extraction
# ==========================================================
def extract_features(video_path):
    if not os.path.exists(POSE_MODEL_PATH):
        print(f"âŒ Pose ëª¨ë¸ íŒŒì¼ ì—†ìŒ â†’ {POSE_MODEL_PATH}")
        return None

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    fps = 30.0 if fps <= 0 else fps
    dt = 1.0 / fps

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # total_time: ì˜ìƒ ì´ ê¸¸ì´ (ì´ˆ)
    total_time = frame_count / fps if fps > 0 else np.nan

    hip_pts, body_sizes = [], []
    hip_visible_flags = []
    frame_idx = 0

    base_options = tasks.BaseOptions(model_asset_path=POSE_MODEL_PATH)
    options = vision.PoseLandmarkerOptions(
        base_options=base_options,
        running_mode=vision.RunningMode.VIDEO,
        num_poses=1,
        min_pose_detection_confidence=0.5,
        min_pose_presence_confidence=0.5,
        min_tracking_confidence=0.5,
        output_segmentation_masks=False,
    )

    with vision.PoseLandmarker.create_from_options(options) as landmarker:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if frame_idx % FRAME_INTERVAL != 0:
                frame_idx += 1
                continue

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mp_frame = mp_image.Image(image_format=mp_image.ImageFormat.SRGB, data=rgb)
            timestamp_ms = int((frame_idx / fps) * 1000)
            result = landmarker.detect_for_video(mp_frame, timestamp_ms)
            if result.pose_landmarks:
                lm = result.pose_landmarks[0]
                left_ok = lm[23].visibility >= VISIBILITY_MIN
                right_ok = lm[24].visibility >= VISIBILITY_MIN
                if left_ok and right_ok:
                    hip_pts.append(center_point((lm[23].x, lm[23].y), (lm[24].x, lm[24].y)))
                    body_sizes.append(robust_body_size_from_landmarks(lm, VISIBILITY_MIN))
                    hip_visible_flags.append(True)
                else:
                    hip_pts.append((np.nan, np.nan))
                    body_sizes.append(np.nan)
                    hip_visible_flags.append(False)
            else:
                hip_pts.append((np.nan, np.nan))
                body_sizes.append(np.nan)
                hip_visible_flags.append(False)
            frame_idx += 1

    cap.release()

    hip_x = np.array([p[0] for p in hip_pts])
    hip_y = np.array([p[1] for p in hip_pts])
    bs = np.array(body_sizes)

    # Hip recognition ratio ì²´í¬ (visibility ê¸°ë°˜, 30% ë¯¸ë§Œì´ë©´ ì œì™¸)
    hip_recognition_ratio = float(np.mean(hip_visible_flags)) if hip_visible_flags else 0.0
    if hip_recognition_ratio < HIP_RECOGNITION_RATIO_MIN:
        print(f"âŒ Hip ì¸ì‹ë¥  {hip_recognition_ratio:.2f} â†’ {os.path.basename(video_path)}")
        return None

    # Body size median ê³„ì‚° (ë³´ê°„ ì „ ì›ë³¸ ìœ íš¨ ê°’ìœ¼ë¡œ)
    valid_bs = bs[np.isfinite(bs)]
    if valid_bs.size == 0:
        print(f"âŒ BodySize ìœ íš¨ê°’ ì—†ìŒ â†’ {os.path.basename(video_path)}")
        return None
    bs_med = float(np.median(valid_bs))
    if not np.isfinite(bs_med) or bs_med <= 1e-6:
        print(f"âŒ BodySize ë¹„ì •ìƒ â†’ {os.path.basename(video_path)}")
        return None

    # Missing data ë³´ê°„
    hip_x = fill_missing(hip_x)
    hip_y = fill_missing(hip_y)
    bs = fill_missing(bs)

    hip_xy = list(zip(hip_x, hip_y))

    # í”½ì…€ ë‹¨ìœ„ ì†ë„ ê³„ì‚°
    hip_v_pixel = velocity_series(hip_xy, dt)

    # Body sizeë¡œ ì •ê·œí™”
    hip_v = hip_v_pixel / bs_med
    hip_v_for_diff = moving_average(hip_v, SMOOTH_WINDOW)
    hip_a = acc_series(hip_v_for_diff, dt)
    hip_j = jerk_series(hip_a, dt)

    # Path length: í”½ì…€ ë‹¨ìœ„ ì´ ì´ë™ê±°ë¦¬ë¥¼ body sizeë¡œ ì •ê·œí™”
    path_pixel = np.sum(hip_v_pixel * dt)
    path = path_pixel / bs_med

    feats = {
        "id": extract_id_and_label(video_path)[0],
        "label": extract_id_and_label(video_path)[1],
        "total_time": total_time,
        "body_size_median": bs_med,

        # Fluency (ìœ ì°½ì„±) - ëª¨ë‘ body_sizeë¡œ ì •ê·œí™”ë¨
        "fluency_hip_velocity_mean": float(np.mean(hip_v)),
        "fluency_hip_velocity_max": float(np.max(hip_v)),
        "fluency_hip_acc_mean": float(np.mean(np.abs(hip_a))),
        "fluency_hip_acc_max": float(np.max(np.abs(hip_a))),
        "fluency_hip_jerk_mean": float(np.mean(np.abs(hip_j))),
        "fluency_hip_jerk_max": float(np.max(np.abs(hip_j))),
        "fluency_hip_jerk_rms": float(np.sqrt(np.mean(hip_j ** 2))),
        "fluency_hip_path_length": float(path),

        # Stability (ì•ˆì •ì„±) - ëª¨ë‘ body_sizeë¡œ ì •ê·œí™”ë¨
        "stability_hip_velocity_sd": float(np.std(hip_v)),
        "stability_hip_acc_sd": float(np.std(hip_a)),
        "stability_hip_jerk_sd": float(np.std(hip_j)),
    }

    return feats


# ==========================================================
# 5. MAIN
# ==========================================================
def main():
    files = glob.glob(os.path.join(VIDEO_DIR, "*.mp4")) + \
            glob.glob(os.path.join(VIDEO_DIR, "*.mov"))

    if not files:
        print("âŒ ë¶„ì„í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“ ì´ {len(files)}ê°œ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘\n")

    success_count = 0
    fail_count = 0

    for idx, video_path in enumerate(files, 1):
        base = os.path.splitext(os.path.basename(video_path))[0]
        out_path = os.path.join(OUTPUT_DIR, f"{base}.xlsx")

        print(f"[{idx}/{len(files)}] {os.path.basename(video_path)}")
        feats = extract_features(video_path)

        if feats is None:
            print(f"  âŒ Feature ì¶”ì¶œ ì‹¤íŒ¨ â†’ {video_path}\n")
            fail_count += 1
            continue

        df = pd.DataFrame([feats])
        df.to_excel(out_path, index=False)
        print(f"  âœ… ì €ì¥ ì™„ë£Œ: {base}.xlsx\n")
        success_count += 1

    print("=" * 60)
    print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {success_count}ê°œ | ì‹¤íŒ¨: {fail_count}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
